{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2399811c-6649-448d-9845-61f62a92147c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordllama\n",
      "  Downloading wordllama-0.3.2.post0-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy in d:\\projects\\langchain_rag\\.venv\\lib\\site-packages (from wordllama) (1.26.4)\n",
      "Collecting safetensors (from wordllama)\n",
      "  Downloading safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tokenizers in d:\\projects\\langchain_rag\\.venv\\lib\\site-packages (from wordllama) (0.20.0)\n",
      "Collecting toml (from wordllama)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: pydantic>=2 in d:\\projects\\langchain_rag\\.venv\\lib\\site-packages (from wordllama) (2.9.2)\n",
      "Requirement already satisfied: requests in d:\\projects\\langchain_rag\\.venv\\lib\\site-packages (from wordllama) (2.32.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\projects\\langchain_rag\\.venv\\lib\\site-packages (from pydantic>=2->wordllama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in d:\\projects\\langchain_rag\\.venv\\lib\\site-packages (from pydantic>=2->wordllama) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in d:\\projects\\langchain_rag\\.venv\\lib\\site-packages (from pydantic>=2->wordllama) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\projects\\langchain_rag\\.venv\\lib\\site-packages (from requests->wordllama) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\projects\\langchain_rag\\.venv\\lib\\site-packages (from requests->wordllama) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\projects\\langchain_rag\\.venv\\lib\\site-packages (from requests->wordllama) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\projects\\langchain_rag\\.venv\\lib\\site-packages (from requests->wordllama) (2024.8.30)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in d:\\projects\\langchain_rag\\.venv\\lib\\site-packages (from tokenizers->wordllama) (0.25.0)\n",
      "Requirement already satisfied: filelock in d:\\projects\\langchain_rag\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->wordllama) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\projects\\langchain_rag\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->wordllama) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\projects\\langchain_rag\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->wordllama) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\projects\\langchain_rag\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->wordllama) (6.0.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\projects\\langchain_rag\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->wordllama) (4.66.5)\n",
      "Requirement already satisfied: colorama in d:\\projects\\langchain_rag\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers->wordllama) (0.4.6)\n",
      "Downloading wordllama-0.3.2.post0-cp312-cp312-win_amd64.whl (16.9 MB)\n",
      "   ---------------------------------------- 0.0/16.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/16.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/16.9 MB 1.9 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 0.8/16.9 MB 1.8 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 1.3/16.9 MB 1.8 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 1.6/16.9 MB 1.6 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 1.8/16.9 MB 1.6 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 2.1/16.9 MB 1.6 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 2.6/16.9 MB 1.7 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 2.9/16.9 MB 1.7 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 3.4/16.9 MB 1.7 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 3.7/16.9 MB 1.7 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 4.2/16.9 MB 1.7 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 4.5/16.9 MB 1.7 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 5.0/16.9 MB 1.7 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 5.2/16.9 MB 1.7 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 5.8/16.9 MB 1.7 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 6.0/16.9 MB 1.7 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 6.3/16.9 MB 1.7 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 6.8/16.9 MB 1.7 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 7.1/16.9 MB 1.7 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 7.3/16.9 MB 1.7 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 7.9/16.9 MB 1.7 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 8.1/16.9 MB 1.7 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 8.4/16.9 MB 1.7 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 8.7/16.9 MB 1.7 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 8.9/16.9 MB 1.6 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 9.2/16.9 MB 1.6 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 9.7/16.9 MB 1.6 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 10.0/16.9 MB 1.6 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 10.2/16.9 MB 1.6 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 10.7/16.9 MB 1.7 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 11.0/16.9 MB 1.7 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 11.5/16.9 MB 1.7 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 11.8/16.9 MB 1.7 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 12.1/16.9 MB 1.7 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 12.6/16.9 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 12.8/16.9 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 13.4/16.9 MB 1.7 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 13.6/16.9 MB 1.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 13.9/16.9 MB 1.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 14.4/16.9 MB 1.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 14.7/16.9 MB 1.7 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 15.2/16.9 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 15.5/16.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 16.0/16.9 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 16.3/16.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.5/16.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.8/16.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.9/16.9 MB 1.6 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Installing collected packages: toml, safetensors, wordllama\n",
      "Successfully installed safetensors-0.4.5 toml-0.10.2 wordllama-0.3.2.post0\n"
     ]
    }
   ],
   "source": [
    "%pip install wordllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26ad408a-3015-46b3-8bdf-e364d9596ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.embeddings.embeddings import Embeddings\n",
    "from wordllama import WordLlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8ce0b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordLlamaEmbeddings(Embeddings):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.wl = WordLlama.load()\n",
    "    \n",
    "    def embed_query(self, text):\n",
    "        return self.embed_documents(texts=[text])[0]\n",
    "    \n",
    "    def embed_documents(self, texts):\n",
    "        return self.wl.embed(texts).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97619192",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl_embedding = WordLlamaEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1b272f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0680389404296875,\n",
       "  0.3641204833984375,\n",
       "  -0.06667518615722656,\n",
       "  0.1931915283203125,\n",
       "  0.0122528076171875,\n",
       "  0.011775970458984375,\n",
       "  -0.17281341552734375,\n",
       "  -0.0818634033203125,\n",
       "  0.04052734375,\n",
       "  -0.29278564453125,\n",
       "  -0.141845703125,\n",
       "  0.188720703125,\n",
       "  0.1627960205078125,\n",
       "  0.0649566650390625,\n",
       "  -0.11504605412483215,\n",
       "  -0.10194921493530273,\n",
       "  0.026763916015625,\n",
       "  -0.30761146545410156,\n",
       "  -0.144775390625,\n",
       "  -0.0410919189453125,\n",
       "  0.2548408508300781,\n",
       "  -0.003597259521484375,\n",
       "  0.056610107421875,\n",
       "  0.16131591796875,\n",
       "  0.08602142333984375,\n",
       "  -0.0589447021484375,\n",
       "  0.15967178344726562,\n",
       "  -0.0382232666015625,\n",
       "  -0.0177154541015625,\n",
       "  -0.053466796875,\n",
       "  0.042510986328125,\n",
       "  0.21312332153320312,\n",
       "  -0.047400474548339844,\n",
       "  -0.08856010437011719,\n",
       "  0.151275634765625,\n",
       "  0.04498291015625,\n",
       "  -0.0533599853515625,\n",
       "  0.235076904296875,\n",
       "  0.0597076416015625,\n",
       "  -0.2534027099609375,\n",
       "  0.11663055419921875,\n",
       "  -0.160980224609375,\n",
       "  0.14080810546875,\n",
       "  0.1207122802734375,\n",
       "  0.1858367919921875,\n",
       "  0.03917694091796875,\n",
       "  -0.115997314453125,\n",
       "  0.054595947265625,\n",
       "  0.1216888427734375,\n",
       "  -0.04558372497558594,\n",
       "  -0.0449066162109375,\n",
       "  0.0961456298828125,\n",
       "  0.171539306640625,\n",
       "  0.03179931640625,\n",
       "  0.12376022338867188,\n",
       "  0.13568115234375,\n",
       "  -0.039825439453125,\n",
       "  -0.07170867919921875,\n",
       "  -0.102813720703125,\n",
       "  -0.06331729888916016,\n",
       "  0.06475830078125,\n",
       "  -0.1102142333984375,\n",
       "  0.23586273193359375,\n",
       "  0.08537483215332031,\n",
       "  -0.2511634826660156,\n",
       "  0.039180755615234375,\n",
       "  -0.01507568359375,\n",
       "  0.19698143005371094,\n",
       "  -0.008331298828125,\n",
       "  -0.16253662109375,\n",
       "  -0.1542816162109375,\n",
       "  -0.08601760864257812,\n",
       "  -0.1497039794921875,\n",
       "  0.0697784423828125,\n",
       "  -0.03002166748046875,\n",
       "  0.016445159912109375,\n",
       "  -0.26593017578125,\n",
       "  -0.196136474609375,\n",
       "  0.11737823486328125,\n",
       "  -0.07021331787109375,\n",
       "  -0.1614837646484375,\n",
       "  -0.2542572021484375,\n",
       "  0.0634307861328125,\n",
       "  -0.134613037109375,\n",
       "  -0.0765533447265625,\n",
       "  0.010467529296875,\n",
       "  -0.036830902099609375,\n",
       "  0.145263671875,\n",
       "  -0.049224853515625,\n",
       "  -0.11905670166015625,\n",
       "  0.228424072265625,\n",
       "  -0.02210235595703125,\n",
       "  0.120574951171875,\n",
       "  -0.17829513549804688,\n",
       "  0.13848876953125,\n",
       "  0.0385894775390625,\n",
       "  0.0475311279296875,\n",
       "  0.07311248779296875,\n",
       "  0.01290130615234375,\n",
       "  -0.197906494140625,\n",
       "  0.136505126953125,\n",
       "  -0.10162734985351562,\n",
       "  0.175994873046875,\n",
       "  0.252777099609375,\n",
       "  0.12670135498046875,\n",
       "  -0.0046844482421875,\n",
       "  0.3170318603515625,\n",
       "  -0.0643463134765625,\n",
       "  -0.05844879150390625,\n",
       "  0.120147705078125,\n",
       "  -0.366363525390625,\n",
       "  -0.21992874145507812,\n",
       "  0.058544158935546875,\n",
       "  -0.0959320068359375,\n",
       "  -0.0680694580078125,\n",
       "  -0.0039215087890625,\n",
       "  0.0626068115234375,\n",
       "  -0.09282684326171875,\n",
       "  0.0258636474609375,\n",
       "  0.245208740234375,\n",
       "  0.04010009765625,\n",
       "  0.025350570678710938,\n",
       "  -0.03814697265625,\n",
       "  0.1106414794921875,\n",
       "  0.0807037353515625,\n",
       "  0.059314727783203125,\n",
       "  0.01593017578125,\n",
       "  0.24636459350585938,\n",
       "  -0.07080078125,\n",
       "  -0.0317840576171875,\n",
       "  0.13668060302734375,\n",
       "  0.04622650146484375,\n",
       "  0.253814697265625,\n",
       "  -0.0662384033203125,\n",
       "  -0.0287017822265625,\n",
       "  0.0638885498046875,\n",
       "  0.20782470703125,\n",
       "  0.04911041259765625,\n",
       "  0.072265625,\n",
       "  -0.107757568359375,\n",
       "  0.178314208984375,\n",
       "  0.1360015869140625,\n",
       "  0.13251018524169922,\n",
       "  -0.0655050277709961,\n",
       "  0.1793212890625,\n",
       "  0.1192169189453125,\n",
       "  0.05810070037841797,\n",
       "  0.067779541015625,\n",
       "  0.2462005615234375,\n",
       "  0.09246826171875,\n",
       "  0.020430564880371094,\n",
       "  0.1574249267578125,\n",
       "  0.1278247833251953,\n",
       "  -0.028778076171875,\n",
       "  -0.0070953369140625,\n",
       "  -0.04034423828125,\n",
       "  0.0953369140625,\n",
       "  0.29224759340286255,\n",
       "  0.07686614990234375,\n",
       "  -0.0172882080078125,\n",
       "  -0.2430267333984375,\n",
       "  0.02228546142578125,\n",
       "  -0.0244598388671875,\n",
       "  0.10022258758544922,\n",
       "  0.11187744140625,\n",
       "  0.032073974609375,\n",
       "  -0.01898956298828125,\n",
       "  0.0753326416015625,\n",
       "  -0.23587799072265625,\n",
       "  0.175872802734375,\n",
       "  0.12832260131835938,\n",
       "  0.13813018798828125,\n",
       "  -0.016143798828125,\n",
       "  -0.11224365234375,\n",
       "  -0.089630126953125,\n",
       "  -0.18215560913085938,\n",
       "  -0.1448822021484375,\n",
       "  -0.047088623046875,\n",
       "  0.064605712890625,\n",
       "  0.0420989990234375,\n",
       "  0.1439361572265625,\n",
       "  0.0869903564453125,\n",
       "  -0.02911376953125,\n",
       "  -0.039714813232421875,\n",
       "  -0.0698699951171875,\n",
       "  0.117645263671875,\n",
       "  -0.1320037841796875,\n",
       "  -0.16085052490234375,\n",
       "  0.0462799072265625,\n",
       "  0.0275726318359375,\n",
       "  0.1543426513671875,\n",
       "  -0.11212921142578125,\n",
       "  -0.028411865234375,\n",
       "  0.069671630859375,\n",
       "  0.0658111572265625,\n",
       "  0.15539932250976562,\n",
       "  0.0857696533203125,\n",
       "  -0.1583404541015625,\n",
       "  -0.02738189697265625,\n",
       "  -0.39666748046875,\n",
       "  -0.28659820556640625,\n",
       "  0.05712890625,\n",
       "  0.017223358154296875,\n",
       "  -0.1697998046875,\n",
       "  -0.2205810546875,\n",
       "  0.123016357421875,\n",
       "  -0.0803070068359375,\n",
       "  -0.1272754669189453,\n",
       "  -0.10002899169921875,\n",
       "  -0.11411285400390625,\n",
       "  -0.041144371032714844,\n",
       "  -0.07200241088867188,\n",
       "  -0.0966033935546875,\n",
       "  0.06844329833984375,\n",
       "  0.22401189804077148,\n",
       "  -0.076690673828125,\n",
       "  -0.240447998046875,\n",
       "  0.1021575927734375,\n",
       "  -0.019761085510253906,\n",
       "  -0.01766204833984375,\n",
       "  0.08640289306640625,\n",
       "  -0.13715362548828125,\n",
       "  0.23114013671875,\n",
       "  -0.066619873046875,\n",
       "  -0.16748046875,\n",
       "  -0.225494384765625,\n",
       "  0.00890350341796875,\n",
       "  0.0853729248046875,\n",
       "  0.139068603515625,\n",
       "  -0.1113739013671875,\n",
       "  -0.01627349853515625,\n",
       "  -0.00445556640625,\n",
       "  -0.11870574951171875,\n",
       "  -0.0573577880859375,\n",
       "  -0.032684326171875,\n",
       "  0.13983154296875,\n",
       "  0.01273345947265625,\n",
       "  -0.121856689453125,\n",
       "  0.042819976806640625,\n",
       "  -0.0280914306640625,\n",
       "  -0.191162109375,\n",
       "  -0.16927146911621094,\n",
       "  -0.2463531494140625,\n",
       "  -0.151519775390625,\n",
       "  0.08513641357421875,\n",
       "  -0.12652587890625,\n",
       "  -0.1083984375,\n",
       "  0.34870147705078125,\n",
       "  -0.148101806640625,\n",
       "  -0.0922698974609375,\n",
       "  -0.163909912109375,\n",
       "  -0.067779541015625,\n",
       "  0.1155242919921875,\n",
       "  -0.0829315185546875,\n",
       "  -0.02569580078125,\n",
       "  -0.055633544921875]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl_embedding.embed_documents('How are you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44af911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9350375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_API_KEY'] = getpass.getpass('LANGCHAIN_API_KEY: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7c69bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "410716ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://llamaimodel.com/requirements/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=('stats-table')\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88eab36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "759f3ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorstore.delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d51aa25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "vectorstore = Chroma.from_documents(documents=splits, embedding=wl_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aca79d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e964123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model='phi3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90c1f092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fab8cf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The article is about the detailed specifications of various versions (3.1, 70B, and 405B) of Llama AI models across different parameters such as context length, multilingual support, hardware requirements for CPUs and RAM, GPU options with associated disk space needs, estimated memory requirements in higher and lower precision modes along with software dependencies including operating systems, programming language (Python), frameworks (PyTorch or TensorFlow), and libraries. Each model specifies these details to guide users on the necessary setup required to run the respective Llama AI models effectively for tasks like natural language processing where multilingual support is also considered across eight languages in this context.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What's the article about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e278f9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
